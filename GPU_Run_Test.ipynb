{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "GPU Run Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nzamski/WaterRecognition/blob/master/GPU_Run_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PXcG0S0ywAC"
      },
      "source": [
        "### Imports"
      ],
      "id": "5PXcG0S0ywAC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c30b7c9e-a5fe-4aba-b0a1-7c245c1624c2"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as f\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from cv2 import imread\n",
        "from pathlib import Path\n",
        "from random import shuffle, seed\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "id": "c30b7c9e-a5fe-4aba-b0a1-7c245c1624c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGqHt3mvy3zR"
      },
      "source": [
        "### Experiments"
      ],
      "id": "EGqHt3mvy3zR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs03n2z4o4Pz"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "hs03n2z4o4Pz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEFajy1gw4nR",
        "outputId": "f2a38456-8a1f-408a-dbd0-5b1b4e328dbf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "aEFajy1gw4nR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Aug 22 09:57:15 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    32W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1KZxCpBiyDS3",
        "outputId": "fc5d7a6f-5597-4b51-c1ea-17357a70de7b"
      },
      "source": [
        "os.getcwd()"
      ],
      "id": "1KZxCpBiyDS3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4iXWW5tyE96",
        "outputId": "560ddd85-ef7e-4b7f-89bb-230cb3920954"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "H4iXWW5tyE96",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJwsx2e8ydoa"
      },
      "source": [
        "### Algorithm Run Tests"
      ],
      "id": "BJwsx2e8ydoa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-yQ0bygqwsp"
      },
      "source": [
        "class Hidden1(nn.Module):\n",
        "    # define the model\n",
        "    def __init__(self, length, hidden_size, activation):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.flat = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(length * length * 3, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    # set activation functions for the layers\n",
        "    def forward(self, x):\n",
        "        x = self.flat(x)\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "id": "J-yQ0bygqwsp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pzDqXPN032H"
      },
      "source": [
        "# set a fixed seed to shuffle paths by\n",
        "seed(42)\n",
        "\n",
        "\n",
        "def get_train_test_paths(test_ratio: float = 0.2):\n",
        "    # extract the data from the dataset folder\n",
        "    files = [file_name for file_name in Path(os.getcwd()+os.sep+'Water Bodies Dataset'+os.sep+'Images').rglob(\"*.jpg\")]\n",
        "    # randomize the order of the data\n",
        "    random.shuffle(files)\n",
        "    # separate test and train files\n",
        "    first_train = int(test_ratio * len(files))\n",
        "    test_path = files[:first_train]\n",
        "    train_path = files[first_train:]\n",
        "    return train_path, test_path\n",
        "\n",
        "\n",
        "def load_image(file_name):\n",
        "    # get image path and return as array\n",
        "    img = Image.open(file_name)\n",
        "    img.load()\n",
        "    data = np.asarray(img, dtype=\"int32\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_mask_path(file_path):\n",
        "    # gets source image path, returns mask path\n",
        "    file_path = str(file_path).replace('Images', 'Masks')\n",
        "    return file_path\n",
        "\n",
        "\n",
        "def load_y(file_path):\n",
        "    mask_path = get_mask_path(file_path)\n",
        "    raw_image = imread(mask_path, 0)\n",
        "    # convert pixel colors to absolute black & white\n",
        "    binary_array = (raw_image < 128).astype(int)\n",
        "    return binary_array\n",
        "\n",
        "\n",
        "class FileLoader:\n",
        "    def __init__(self, path, length: int, load_both: bool = True):\n",
        "        self.length = length\n",
        "        self.load_both = load_both\n",
        "        # source image variables initialization\n",
        "        self.source_image = load_image(path)\n",
        "        self.source_current_x, self.source_current_y = -1, 0  # -1 is a temp starting point\n",
        "        self.source_max_x = self.source_image.shape[0] - self.length\n",
        "        self.source_max_y = self.source_image.shape[1] - self.length\n",
        "        # mask image variables initialization\n",
        "        self.mask_image = load_y(path) if load_both else None\n",
        "        self.mask_current_x = int((length - 1) / 2) - 1 if load_both else None  # -1 is a temp starting point\n",
        "        self.mask_current_y = int((length - 1) / 2) if load_both else None\n",
        "        self.mask_max_x = self.mask_image.shape[0] - int((length - 1) / 2) if load_both else None\n",
        "        self.mask_max_y = self.mask_image.shape[1] - int((length - 1) / 2) if load_both else None\n",
        "\n",
        "    def advance_source_index(self):\n",
        "        if self.source_current_x < self.source_max_x - self.length:\n",
        "            # the slice doesn't exceed the columns\n",
        "            self.source_current_x += 1\n",
        "        else:\n",
        "            # go to the next row and reset the x\n",
        "            self.source_current_x = 0\n",
        "            if self.source_current_y < self.source_max_y - self.length:\n",
        "                self.source_current_y += 1\n",
        "            else:\n",
        "                # finished going through image\n",
        "                return None\n",
        "        return self.source_current_x, self.source_current_y\n",
        "\n",
        "    def advance_mask_index(self):\n",
        "        if self.mask_current_x < self.mask_max_x:  # no need in self.length, right?\n",
        "            # the tag doesn't exceed the columns\n",
        "            self.mask_current_x += 1\n",
        "        else:\n",
        "            # go to the next row and reset the x\n",
        "            self.mask_current_x = 0\n",
        "            if self.mask_current_y < self.mask_max_y:  # same\n",
        "                self.mask_current_y += 1\n",
        "            else:\n",
        "                # finished going through image\n",
        "                return None\n",
        "        return self.mask_current_x, self.mask_current_y\n",
        "\n",
        "    def get_next(self):\n",
        "        # returns the next slice (+ tag)\n",
        "        source_indices = self.advance_source_index()\n",
        "        if source_indices is None:\n",
        "            return None\n",
        "        else:\n",
        "            source_x, source_y = source_indices\n",
        "            source_slice = self.source_image[\n",
        "                           source_x:source_x + self.length,\n",
        "                           source_y:source_y + self.length,\n",
        "                           :]\n",
        "        if self.load_both:\n",
        "            mask_x, mask_y = self.advance_mask_index()\n",
        "            mask_tag = self.mask_image[mask_x, mask_y]\n",
        "            return source_slice, mask_tag\n",
        "        return source_slice\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, path_list, length, batch_size: int = 4, load_both: bool = True, return_file_name: bool = False):\n",
        "        self.path_list = path_list\n",
        "        shuffle(self.path_list)\n",
        "        self.length = length\n",
        "        self.current_file_index = 0\n",
        "        # expected batch size smaller than number of paths\n",
        "        self.batch_size = batch_size\n",
        "        self.load_both = load_both\n",
        "        self.return_file_name = return_file_name\n",
        "        self.active_file_loaders, self.file_names = self.setup_active_file_loaders()\n",
        "        self.file_index = batch_size\n",
        "        self.max_file_index = len(self.path_list)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def setup_active_file_loaders(self):\n",
        "        active_file_loaders, file_names = list(), list()\n",
        "        for _ in tqdm(range(self.batch_size)):\n",
        "            new_loader, file_name = self.get_next_loader()\n",
        "            active_file_loaders.append(new_loader)\n",
        "            file_names.append(file_name)\n",
        "        return active_file_loaders, file_names\n",
        "\n",
        "    def get_next_loader(self):\n",
        "        # return the next file loader (by order) and its path\n",
        "        if self.current_file_index < len(self.path_list):\n",
        "            path = self.path_list[self.current_file_index]\n",
        "            new_loader = FileLoader(path, self.length, self.load_both)\n",
        "            self.current_file_index += 1\n",
        "            return new_loader, path\n",
        "        return None, None\n",
        "\n",
        "    def __next__(self):\n",
        "        # initiate lists to store a batch of data\n",
        "        x_batch, y_batch = list(), list()\n",
        "        for loader_index, data_loader in enumerate(self.active_file_loaders):\n",
        "            # get next potential data loader\n",
        "            loaded_result = data_loader.get_next()\n",
        "            # end of current loader\n",
        "            if loaded_result is None:\n",
        "                new_loader, file_name = self.get_next_loader()\n",
        "                # finished going through path list\n",
        "                if new_loader is None:\n",
        "                    del(self.active_file_loaders[loader_index])\n",
        "                    del(self.file_names[loader_index])\n",
        "                    # end of all loaders in active file loaders\n",
        "                    if len(self.active_file_loaders) == 0:\n",
        "                        raise StopIteration()\n",
        "                # didn't finished going through all the data yet\n",
        "                else:\n",
        "                    data_loader = new_loader\n",
        "                    self.active_file_loaders[loader_index] = data_loader\n",
        "                    self.file_names[loader_index] = file_name\n",
        "                    loaded_result = data_loader.get_next()\n",
        "            # finished a batch\n",
        "            if loaded_result is not None:\n",
        "                x_batch.append(loaded_result[0])\n",
        "                if self.load_both:\n",
        "                    y_batch.append(loaded_result[1])\n",
        "        # return all x, y batches\n",
        "        results = [x_batch]\n",
        "        if self.load_both:\n",
        "            results.append(y_batch)\n",
        "        if self.return_file_name:\n",
        "            results.append(self.file_names)\n",
        "        return results"
      ],
      "id": "9pzDqXPN032H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e247927c-3552-4711-8cd5-76c19d8a6fc1"
      },
      "source": [
        "def get_train_test_paths(test_ratio: float = 0.2):\n",
        "    # extract the data from the dataset folder\n",
        "    files = [file_name for file_name in Path(os.getcwd()+os.sep+'drive'+os.sep+'MyDrive'+os.sep+'Water Bodies Dataset'+os.sep+'Images').rglob(\"*.jpg\")]\n",
        "    # randomize the order of the data\n",
        "    shuffle(files)\n",
        "    # separate test and train files\n",
        "    first_train = int(test_ratio * len(files))\n",
        "    test_path = files[:first_train]\n",
        "    train_path = files[first_train:]\n",
        "    return train_path, test_path\n",
        "\n",
        "\n",
        "def get_mask_path(file_path):\n",
        "    # disassemble and assemble data path to return mask path\n",
        "    wdr = os.getcwd()+os.sep+'drive'+os.sep+'MyDrive'+os.sep+'Water Bodies Dataset'+os.sep+'Masks'\n",
        "    file_path = str(file_path).split(os.sep)[-1]\n",
        "    mask_path = wdr + os.sep + file_path\n",
        "    return mask_path\n",
        "\n",
        "\n",
        "def load_image(file_name):\n",
        "    # get image path and return as array\n",
        "    img = Image.open(file_name)\n",
        "    img.load()\n",
        "    data = np.asarray(img, dtype=\"int32\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def split_to_squares(image_path, length):\n",
        "    # get image array from the path\n",
        "    rgb_array = load_image(image_path)\n",
        "    # store image height and width (in pixels)\n",
        "    max_x, max_y, _ = rgb_array.shape\n",
        "    # initiate list for image slices\n",
        "    slices = []\n",
        "    # move along the image and save every square to the list\n",
        "    for corner_x in range(max_x - length + 1):  # why not +2 ?\n",
        "        for corner_y in range(max_y - length + 1):\n",
        "            # append the squared matrix to the list\n",
        "            sub = rgb_array[corner_x:corner_x+length, corner_y:corner_y+length, :]\n",
        "            slices.append(sub)\n",
        "    return slices\n",
        "\n",
        "\n",
        "def get_y(image_path, length):  # expected odd length\n",
        "    # get mask from path\n",
        "    binary_array = imread(image_path, 0)\n",
        "    # store mask height and width (in pixels)\n",
        "    max_x, max_y = binary_array.shape\n",
        "    # convert pixel colors to absolute black & white\n",
        "    binary_array = (binary_array < 128).astype(int)\n",
        "    # initiate list for mask slices\n",
        "    tags = []\n",
        "    # move along the mask and save every square to the list\n",
        "    for x in range(int((length - 1) / 2), max_x - int((length - 1) / 2)):\n",
        "        for y in range(int((length - 1) / 2), max_y - int((length - 1) / 2)):\n",
        "            # append the pixel to the list\n",
        "            tag = binary_array[x, y]\n",
        "            tags.append(tag)\n",
        "    return tags\n",
        "\n",
        "\n",
        "def get_x_y(file_path, length):\n",
        "    X = split_to_squares(file_path, length)\n",
        "    mask_path = get_mask_path(file_path)\n",
        "    y = get_y(mask_path, length)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def fit_model(model, model_parameters, loss_function, batch_size, optimizer, input_image_length):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # retrieve train and test files\n",
        "    train, test = get_train_test_paths()\n",
        "    train_loader = DataLoader(train, input_image_length, batch_size)\n",
        "    test_loader = DataLoader(test, input_image_length, batch_size)\n",
        "    # initiate a list for loss accumulation\n",
        "    losses = list()\n",
        "    # assign the model\n",
        "    model = model(*model_parameters).to(device)\n",
        "    # set a loss function\n",
        "    criterion = loss_function()\n",
        "    # set an optimizer\n",
        "    optimizer = optimizer(model.parameters(), lr=0.001)\n",
        "    model.train()\n",
        "    for epoch in range(10):\n",
        "      epoch_start = datetime.now()\n",
        "      epoch_loss = 0\n",
        "      # iterate through all data pairs\n",
        "      for x, y in tqdm(train_loader):\n",
        "        # convert input pixel to tensor and flatten\n",
        "        x = torch.tensor(x).float().to(device)\n",
        "        # convert target to tensor\n",
        "        tag = torch.tensor(y, dtype=torch.long).view(-1).to(device)\n",
        "        # set all gradients to to zero\n",
        "        optimizer.zero_grad()\n",
        "        prediction = model(x)\n",
        "        # activate cross entropy, calculate loss\n",
        "        loss = criterion(prediction, tag)\n",
        "        # back propagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # update into current loss\n",
        "        epoch_loss += loss.item()\n",
        "      epoch_end = datetime.now()\n",
        "      epoch_seconds = (epoch_end - epoch_start).total_seconds()\n",
        "      model.eval()\n",
        "      predicted, real = list(), list()\n",
        "      for x, y in test_loader:\n",
        "        real += y\n",
        "        probabilities = model(x)\n",
        "        _, batch_predicted = torch.max(probabilities, 1)\n",
        "        batch_predicted = list(batch_predicted.view(-1))\n",
        "        predicted += batch_predicted\n",
        "      accuracy = accuracy_score(real, predicted)\n",
        "      f1 = f1_score(real, predicted)\n",
        "    \n",
        "\n",
        "      df = pd.DataFrame({'Model Name':['Hidden1'],\n",
        "                         'Iteration':[epoch],\n",
        "                         'Hyperparameters':[f'''\"input_image_length\": 5\n",
        "                         \"hidden_layer_size\": 10\n",
        "                         \"activation\": \"f.relu\"\n",
        "                         \"optimizer\": \"optim.Adam\"\n",
        "                         \"loss_function\": \"nn.CrossEntropyLoss\"'''],\n",
        "                         'Loss':[running_loss],\n",
        "                         'Accuracy':[accuracy],\n",
        "                         'F1':[f1],\n",
        "                         'Iteration Training Seconds':[epoch_seconds]})\n",
        "      df.to_csv('drive/MyDrive/water_bodies_results.csv', index=False, mode='a', header=False)\n",
        "      print(df)"
      ],
      "id": "e247927c-3552-4711-8cd5-76c19d8a6fc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGez7HhmxlBB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "5fb49281-a4ff-4651-f97e-d1cea906790f"
      },
      "source": [
        "model = Hidden1\n",
        "batch_size = 256\n",
        "input_image_length = 5\n",
        "hidden_layer_size = 10\n",
        "activation = f.relu\n",
        "model_parameters = (input_image_length, hidden_layer_size, activation)\n",
        "optimizer = optim.Adam\n",
        "loss_function = nn.CrossEntropyLoss\n",
        "fit_model(model, model_parameters, loss_function, batch_size, optimizer, input_image_length)"
      ],
      "id": "NGez7HhmxlBB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 256/256 [00:07<00:00, 35.52it/s]\n",
            "100%|██████████| 256/256 [00:05<00:00, 46.92it/s]\n",
            "42721it [10:04, 70.68it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-209e46849e7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_image_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-fd94e871e6e2>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, model_parameters, loss_function, batch_size, optimizer, input_image_length)\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# convert input pixel to tensor and flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;31m# convert target to tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}