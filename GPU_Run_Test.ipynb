{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "GPU Run Test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PXcG0S0ywAC"
      },
      "source": [
        "### Imports"
      ],
      "id": "5PXcG0S0ywAC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c30b7c9e-a5fe-4aba-b0a1-7c245c1624c2"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as f\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from cv2 import imread\n",
        "from pathlib import Path\n",
        "from random import shuffle"
      ],
      "id": "c30b7c9e-a5fe-4aba-b0a1-7c245c1624c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGqHt3mvy3zR"
      },
      "source": [
        "### Experiments"
      ],
      "id": "EGqHt3mvy3zR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs03n2z4o4Pz"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "hs03n2z4o4Pz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEFajy1gw4nR",
        "outputId": "6a71c38c-9cae-4ad4-b59b-173df7708836"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "aEFajy1gw4nR",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug  2 21:33:39 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kieXCReM1tH0"
      },
      "source": [
        "import cPickle\n",
        "# save the classifier\n",
        "with open('my_dumped_classifier.pkl', 'wb') as fid:\n",
        "    cPickle.dump(gnb, fid)    \n",
        "\n",
        "# load it again\n",
        "with open('my_dumped_classifier.pkl', 'rb') as fid:\n",
        "    gnb_loaded = cPickle.load(fid)"
      ],
      "id": "kieXCReM1tH0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1KZxCpBiyDS3",
        "outputId": "2c8be007-efe0-4213-ac7a-f853426645b8"
      },
      "source": [
        "os.getcwd()"
      ],
      "id": "1KZxCpBiyDS3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4iXWW5tyE96",
        "outputId": "5e6a6ee3-3782-448f-e7f0-bba0d6efb9b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "H4iXWW5tyE96",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5NPA4vqvMvb",
        "outputId": "8b04948f-f033-45b4-fd2a-f841e5acf513"
      },
      "source": [
        "from glob import glob\n",
        "\n",
        "dir_list = [i for i in glob('/content/drive/MyDrive/Water Bodies Dataset/*/*')]\n",
        "dir_list[:10]"
      ],
      "id": "v5NPA4vqvMvb",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Water Bodies Dataset/Images/water_body_8349.jpg',\n",
              " '/content/drive/MyDrive/Water Bodies Dataset/Images/water_body_7917.jpg',\n",
              " '/content/drive/MyDrive/Water Bodies Dataset/Images/water_body_7926.jpg',\n",
              " '/content/drive/MyDrive/Water Bodies Dataset/Images/water_body_7744.jpg',\n",
              " '/content/drive/MyDrive/Water Bodies Dataset/Images/water_body_8087.jpg',\n",
              " '/content/drive/MyDrive/Water Bodies Dataset/Images/water_body_8012.jpg',\n",
              " '/content/drive/MyDrive/Water Bodies Dataset/Images/water_body_7731.jpg',\n",
              " '/content/drive/MyDrive/Water Bodies Dataset/Images/water_body_817.jpg',\n",
              " '/content/drive/MyDrive/Water Bodies Dataset/Images/water_body_769.jpg',\n",
              " '/content/drive/MyDrive/Water Bodies Dataset/Images/water_body_793.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJwsx2e8ydoa"
      },
      "source": [
        "### Algorithm Run Tests"
      ],
      "id": "BJwsx2e8ydoa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-yQ0bygqwsp"
      },
      "source": [
        "class Hidden1(nn.Module):\n",
        "    # define the model\n",
        "    def __init__(self, length, hidden_size, activation):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.fc1 = nn.Linear(length * length * 3, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    # set activation functions for the layers\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "id": "J-yQ0bygqwsp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e247927c-3552-4711-8cd5-76c19d8a6fc1"
      },
      "source": [
        "def get_train_test_paths(test_ratio: float = 0.2):\n",
        "    # extract the data from the dataset folder\n",
        "    files = [file_name for file_name in Path(os.getcwd()+os.sep+'drive'+os.sep+'MyDrive'+os.sep+'Water Bodies Dataset'+os.sep+'Images').rglob(\"*.jpg\")]\n",
        "    # randomize the order of the data\n",
        "    shuffle(files)\n",
        "    # separate test and train files\n",
        "    first_train = int(test_ratio * len(files))\n",
        "    test_path = files[:first_train]\n",
        "    train_path = files[first_train:]\n",
        "    return train_path, test_path\n",
        "\n",
        "\n",
        "def get_mask_path(file_path):\n",
        "    # disassemble and assemble data path to return mask path\n",
        "    wdr = os.getcwd()+os.sep+'drive'+os.sep+'MyDrive'+os.sep+'Water Bodies Dataset'+os.sep+'Masks'\n",
        "    file_path = str(file_path).split(os.sep)[-1]\n",
        "    mask_path = wdr + os.sep + file_path\n",
        "    return mask_path\n",
        "\n",
        "\n",
        "def load_image(file_name):\n",
        "    # get image path and return as array\n",
        "    img = Image.open(file_name)\n",
        "    img.load()\n",
        "    data = np.asarray(img, dtype=\"int32\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def split_to_squares(image_path, length):\n",
        "    # get image array from the path\n",
        "    rgb_array = load_image(image_path)\n",
        "    # store image height and width (in pixels)\n",
        "    max_x, max_y, _ = rgb_array.shape\n",
        "    # initiate list for image slices\n",
        "    slices = []\n",
        "    # move along the image and save every square to the list\n",
        "    for corner_x in range(max_x - length + 1):  # why not +2 ?\n",
        "        for corner_y in range(max_y - length + 1):\n",
        "            # append the squared matrix to the list\n",
        "            sub = rgb_array[corner_x:corner_x+length, corner_y:corner_y+length, :]\n",
        "            slices.append(sub)\n",
        "    return slices\n",
        "\n",
        "\n",
        "def get_y(image_path, length):  # expected odd length\n",
        "    # get mask from path\n",
        "    binary_array = imread(image_path, 0)\n",
        "    # store mask height and width (in pixels)\n",
        "    max_x, max_y = binary_array.shape\n",
        "    # convert pixel colors to absolute black & white\n",
        "    binary_array = (binary_array < 128).astype(int)\n",
        "    # initiate list for mask slices\n",
        "    tags = []\n",
        "    # move along the mask and save every square to the list\n",
        "    for x in range(int((length - 1) / 2), max_x - int((length - 1) / 2)):\n",
        "        for y in range(int((length - 1) / 2), max_y - int((length - 1) / 2)):\n",
        "            # append the pixel to the list\n",
        "            tag = binary_array[x, y]\n",
        "            tags.append(tag)\n",
        "    return tags\n",
        "\n",
        "\n",
        "def get_x_y(file_path, length):\n",
        "    X = split_to_squares(file_path, length)\n",
        "    mask_path = get_mask_path(file_path)\n",
        "    y = get_y(mask_path, length)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def batch_loader(path_list, batch_size: int = 4):\n",
        "    \n",
        "\n",
        "\n",
        "def fit_model(model, model_parameters, loss_function, optimizer, preprocessor, input_image_length):\n",
        "    # retrieve train and test files\n",
        "    train, test = get_train_test_paths()\n",
        "    # initiate a list for loss accumulation\n",
        "    losses = list()\n",
        "    # assign the model\n",
        "    model = model(*model_parameters)\n",
        "    # set a loss function\n",
        "    criterion = loss_function()\n",
        "    # set an optimizer\n",
        "    optimizer = optimizer(model.parameters(), lr=0.001)\n",
        "    # iterate through all data pairs\n",
        "    for path in train:\n",
        "        X, y = preprocessor(path, input_image_length)\n",
        "        # initiate loss variable for current epoch\n",
        "        running_loss = 0\n",
        "        for i, (x, target) in tqdm(enumerate(zip(X, y))):\n",
        "            \n",
        "            # convert input pixel to tensor and flatten\n",
        "            x = torch.flatten(torch.tensor(x)).float()\n",
        "            # convert target to tensor\n",
        "            tag = torch.tensor([target], dtype=torch.long)\n",
        "            # set all gradients to to zero\n",
        "            optimizer.zero_grad()\n",
        "            prediction = model(x).reshape((1, 2))\n",
        "            # activate cross entropy, calculate loss\n",
        "            loss = criterion(prediction, tag)\n",
        "            # back propagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # update into current loss\n",
        "            running_loss += loss.item()\n",
        "            if i % 5_000 == 0:\n",
        "                print(loss.item())\n",
        "        # add current loss to the list\n",
        "        losses.append(running_loss / len(y))\n",
        "    print(losses)"
      ],
      "id": "e247927c-3552-4711-8cd5-76c19d8a6fc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGez7HhmxlBB"
      },
      "source": [
        "model = Hidden1\n",
        "input_image_length = 5\n",
        "hidden_layer_size = 10\n",
        "activation = f.relu\n",
        "model_parameters = (input_image_length, hidden_layer_size, activation)\n",
        "optimizer = optim.Adam\n",
        "loss_function = nn.CrossEntropyLoss\n",
        "preprocessor = get_x_y\n",
        "fit_model(model, model_parameters, loss_function, optimizer, preprocessor, input_image_length)"
      ],
      "id": "NGez7HhmxlBB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxy53_Hdxx0P"
      },
      "source": [
        "a, b = get_train_test_paths()"
      ],
      "id": "xxy53_Hdxx0P",
      "execution_count": null,
      "outputs": []
    }
  ]
}